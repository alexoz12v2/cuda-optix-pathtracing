#pragma once

#include "dmtmacros.h"

#if !defined(__NVCC__) && !defined(__CUDA_ARCH__)
    #include "platform/platform-context.h"
    #include "cuda-wrappers/cuda-wrappers-utils.h"
    #include <algorithm>
    #include <type_traits>

    #include <vector>
    #include <cstddef>
    #include <type_traits>

namespace dmt {
    template <typename... Ts>
    void populateSizesAndAlignments(std::vector<size_t>& sizes, std::vector<size_t>& alignments)
    {
        // Reserve space for efficiency
        sizes.reserve(sizeof...(Ts));
        alignments.reserve(sizeof...(Ts));

        // Fold expression over the comma operator
        ((sizes.push_back(sizeof(Ts)), alignments.push_back(alignof(Ts))), ...);
    }
} // namespace dmt
#endif

#if defined(__CUDA_ARCH__)
// ------------------ Metafunctions ------------------

// -------------------- index helpers --------------------
// clang-format off
template <size_t... Is> struct IndexSequence {};
template <size_t N, size_t... Is> struct MakeIndexSequence : MakeIndexSequence<N-1, N-1, Is...> {};
template <size_t... Is> struct MakeIndexSequence<0, Is...> { using type = IndexSequence<Is...>; };
template <typename... Ts> using IndexSequenceFor = typename MakeIndexSequence<sizeof...(Ts)>::type;
template <size_t N> struct IC { static constexpr size_t value = N; };

// -------------------- type-at helper --------------------
template <size_t I, typename... Ts> struct TypeAt;
template <typename T, typename... Ts> struct TypeAt<0, T, Ts...> { using type = T; };
template <size_t I, typename T, typename... Ts>
struct TypeAt<I, T, Ts...> { using type = typename TypeAt<I-1, Ts...>::type; };

// -------------------- PtrHolder / PtrTuple (stores pointers) --------------------
template <size_t I, typename T> struct PtrHolder { T* ptr; };

template <typename IndexSeq, typename... Ts> struct PtrTupleImpl;
template <size_t... Is, typename... Ts>
struct PtrTupleImpl<IndexSequence<Is...>, Ts...> : PtrHolder<Is, Ts>... {
    __device__ PtrTupleImpl(Ts*... ps) : PtrHolder<Is, Ts>{ps}... {}
    template <size_t I>
    __device__ auto get() -> typename TypeAt<I, Ts...>::type* {
        return this->template PtrHolder<I, typename TypeAt<I, Ts...>::type>::ptr;
    }
};
template <typename... Ts> using PtrTuple = PtrTupleImpl<IndexSequenceFor<Ts...>, Ts...>;

// -------------------- ValueHolder / ValueTuple (stores values) --------------------
template <size_t I, typename T> struct ValueHolder { T val; };

template <typename IndexSeq, typename... Ts> struct ValueTupleImpl;
template <size_t... Is, typename... Ts>
struct ValueTupleImpl<IndexSequence<Is...>, Ts...> : ValueHolder<Is, Ts>... {
    __device__ ValueTupleImpl(Ts const&... vs) : ValueHolder<Is, Ts>{vs}... {}
    template <size_t I>
    __device__ auto get() -> typename TypeAt<I, Ts...>::type const& {
        return this->template ValueHolder<I, typename TypeAt<I, Ts...>::type>::val;
    }
};
template <typename... Ts> using ValueTuple = ValueTupleImpl<IndexSequenceFor<Ts...>, Ts...>;
// clang-format on

// -------------------- Indices apply (calls functor for each index) --------------------
template <size_t N>
struct Indices
{
    template <typename F>
    static __device__ void apply(F& f)
    {
        if constexpr (N > 0)
        {
            Indices<N - 1>::apply(f);
            f.template operator()<N - 1>(); // pass size_t directly
        }
    }
};

// -------------------- functors --------------------
template <typename Queue, typename PtrTupleT>
struct PopDeviceFunctor
{
    Queue*     queue;
    PtrTupleT& outs;
    int        slot;
    __device__ PopDeviceFunctor(Queue* q, PtrTupleT& o, int s) : queue(q), outs(o), slot(s) {}

    template <size_t I>
    __device__ void operator()()
    {
        auto ptr = outs.template get<I>();
        if (ptr)
            *ptr = queue->template buffer<I>()[slot];
    }
};

template <typename Queue, typename ValTupleT>
struct PushDeviceFunctor
{
    Queue*     queue;
    ValTupleT& vals;
    int        slot;
    __device__ PushDeviceFunctor(Queue* q, ValTupleT& v, int s) : queue(q), vals(v), slot(s) {}

    template <size_t I>
    __device__ void operator()()
    {
        queue->template buffer<I>()[slot] = vals.template get<I>();
    }
};


#endif

namespace dmt {
    constexpr __host__ __device__ size_t alignUp(size_t size, size_t alignment)
    {
        return (size + alignment - 1) / alignment * alignment;
    }


    /// Queue stored in unified (managed) memory
    /// Allocation uses the driver API cuMemAllocManaged
    /// The queue header nad the ring buffer elements are placed in the managed allocation
    /// Important: Host and Device must not access concurently
    template <typename T>
#if !defined(__NVCC__) && !defined(__CUDA_ARCH__)
        requires(std::is_trivial_v<T> && std::is_standard_layout_v<T>)
#endif
    struct alignas(T) ManagedQueue
    {
        // metadata: placed in managed memory so both host and device can see it (requires sync)
        int32_t capacity; // number of elements
        int32_t head;     // index for pop. host-only or protected by synchronization protocol
        int32_t tail;     // index for push (next free). ring buffer uses modulo capacity
        int32_t count;    // optional count updated with atomic operations

        // data: following this header is properly aligned data.
        // Layout: [ManagedQueue<T> header][T data[capacity]]

        // device helper: obtains pointer to element storage. Works because alignas ensures proper padding
        __host__ __device__ T* data() { return reinterpret_cast<T*>(this + 1); }

        // --- Host-side APIs (no device concurrency) ---
        /// push from host (non-threadsafe w.r.t device access; caller must ensure no device activity)
#if !defined(__NVCC__) && !defined(__CUDA_ARCH__)
        /// peek from host: non-threadsafe, does not modify head/tail
        /// @warning debug only
        __host__ bool peekHost(int index, T* out)
        {
            if (index < 0 || index >= count)
                return false;
            if (out)
            {
                int slot = (head + index) % capacity;
                *out     = data()[slot];
            }
            return true;
        }

        __host__ bool pushHost(T const& v)
        {
            if (count >= capacity)
                return false;
            // simple ring push (no atomics) -- caller must ensure host-only access
            data()[tail] = v;
            tail         = (tail + 1) % capacity;
            ++count;
            return true;
        }

        /// pop from host (non-threadsafe w.r.t device access; caller must ensure no device activity)
        __host__ bool popHost(T* out)
        {
            if (count <= 0)
                return false;
            if (out)
                *out = data()[head];

            head = (head + 1) % capacity;
            --count;
            return true;
        }
#endif

// --- Device-side APIs (safe for many device threads within a kernel) ---
/// Device pushes are implemented with atomic operations so many threads can push, but host must not access concurrently
#if defined(__CUDA_ARCH__) // necessary as we are not using the nvcc compiler driver
        __device__ bool pushDevice(T const& v)
        {
            // reserve an index atomically and compute slot
            int ticket = atomicAdd(&tail, 1);
            int slot   = ticket % capacity;

            // if the queue was full, decrement count and mark failure
            int oldCount = atomicAdd(&count, 1);
            if (oldCount >= capacity)
            {
                // Note: tail rollback is not trivial in
                // high-concurrency context â€” we accept that tail will have advanced (ticket counter).
                // this might fail if in the meantime some slot gets free
                atomicSub(&count, 1);
                return false;
            }

            // store element into slot
            data()[slot] = v;

            // a device writer may want a release store; CUDA device writes to global memory are visible after store.
            // For simplicity we assume default memory ordering is sufficient for consumer on device or host once synchronization happens.
            return true;
        }

        __device__ bool popDevice(T* out)
        {
            // reserve head ticket, compute slot
            int ticket = atomicAdd(&head, 1);
            int slot   = ticket % capacity;

            int oldCount = atomicSub(&count, 1);
            if (oldCount <= 0)
            {
                // nothing to pop
                atomicAdd(&count, 1);
                return false;
            }

            if (out)
                *out = data()[slot];

            return true;
        }
#endif

        // ---- Allocation / deallocation helpers (host only) ----
#if !defined(__NVCC__) && !defined(__CUDA_ARCH__)
        /// Allocate a ManagedQueue<T> with space for 'cap' elements in managed memory using driver API cuMemAllocManaged.
        /// it is adviseable to use trivial, standard layout types
        /// flags: CU_MEM_ATTACH_GLOBAL or CU_MEM_ATTACH_HOST (driver API values).
        static inline ManagedQueue<T>* allocateManaged(CUDADriverLibrary const& nvApi,
                                                       int                      cap,
                                                       size_t&                  out_bytes,
                                                       uint32_t                 flags = CU_MEM_ATTACH_GLOBAL)
        {
            static_assert(std::is_trivially_copyable<T>::value, "T must be trivially copyable for this queue.");
            size_t const headerSize = sizeof(ManagedQueue<T>);
            // no need to add padding to headerSize if headerSize is aligned to T
            size_t const total = headerSize + static_cast<size_t>(cap) * sizeof(T);

            CUdeviceptr devPtr = 0;
            if (!cudaDriverCall(&nvApi, nvApi.cuMemAllocManaged(&devPtr, total, flags)))
                return nullptr;
            auto* q = std::bit_cast<ManagedQueue<T>*>(devPtr);
            new (q) ManagedQueue<T>();
            q->capacity = cap;
            q->head     = 0;
            q->tail     = 0;
            q->count    = 0;

            out_bytes = total;
            return q;
        }

        static inline void freeManaged(CUDADriverLibrary const& nvApi, ManagedQueue<T>* q)
        {
            auto devPtr = std::bit_cast<CUdeviceptr>(q);
            if (devPtr)
            {
                CUresult res = nvApi.cuMemFree(devPtr);
                if (Context ctx; res != ::CUDA_SUCCESS && ctx.isValid())
                {
                    char const* ptr = nullptr;
                    nvApi.cuGetErrorString(res, &ptr);
                    ctx.error("CUDA Error while freeing queue: {}", std::make_tuple(ptr));
                }
            }
        }
#endif
    };

    //----------------------------------------------------------
    template <typename... Ts>
    struct ManagedMultiQueue
    {
        int32_t capacity; // number of elements
        int32_t head;     // index for pop. host-only or protected by synchronization protocol
        int32_t tail;     // index for push (next free). ring buffer uses modulo capacity
        int32_t count;    // optional count updated with atomic operations


        // --- Host-side APIs ---
#if !defined(__NVCC__) && !defined(__CUDA_ARCH__)

        // allocate all sub-queues in managed memory
        static inline ManagedMultiQueue<Ts...>* allocateManaged(CUDADriverLibrary const& nvApi,
                                                                int                      cap,
                                                                size_t&                  out_bytes,
                                                                uint32_t                 flags = CU_MEM_ATTACH_GLOBAL)
        {
            std::vector<size_t> sizes;
            std::vector<size_t> alignments;
            populateSizesAndAlignments<Ts...>(sizes, alignments);
            size_t const alignment    = *std::ranges::max_element(alignments);
            size_t const headerPadded = alignUp(sizeof(ManagedMultiQueue<Ts...>), alignment);
            size_t       totalSize    = headerPadded;
            out_bytes                 = totalSize;

            for (uint32_t i = 0; i < sizes.size(); ++i)
            {
                if (i != sizes.size() - 1)
                    totalSize += alignUp(sizes[i] * static_cast<size_t>(cap), alignment);
                else
                    totalSize += sizes[i] * static_cast<size_t>(cap);
            }

            CUdeviceptr devPtr = 0;
            if (!cudaDriverCall(&nvApi, nvApi.cuMemAllocManaged(&devPtr, totalSize, flags)))
                return nullptr;

            auto* mmq = std::bit_cast<ManagedMultiQueue<Ts...>*>(devPtr);
            new (mmq) ManagedMultiQueue<Ts...>();
            mmq->capacity = cap;
            mmq->head     = 0;
            mmq->tail     = 0;
            mmq->count    = 0;

            return mmq;
        }

        static inline void freeManaged(CUDADriverLibrary const& nvApi, ManagedMultiQueue<Ts...>* mmq)
        {
            if (!mmq)
                return;

            CUdeviceptr devPtr = reinterpret_cast<CUdeviceptr>(mmq);
            CUresult    res    = nvApi.cuMemFree(devPtr);

            if (Context ctx; res != CUDA_SUCCESS && ctx.isValid())
            {
                char const* errStr = nullptr;
                nvApi.cuGetErrorString(res, &errStr);
                ctx.error("CUDA Error while freeing ManagedMultiQueue: {}", std::make_tuple(errStr));
            }
        }

        // Compute pointer to i-th buffer (T0, T1, ...) in managed memory
        template <std::size_t Index>
        auto* buffer()
        {
            std::vector<size_t> sizes;
            std::vector<size_t> alignments;
            populateSizesAndAlignments<Ts...>(sizes, alignments);

            size_t const maxAlignment = *std::ranges::max_element(alignments);
            size_t       offset       = alignUp(sizeof(ManagedMultiQueue<Ts...>), maxAlignment);

            for (std::size_t i = 0; i < Index; ++i)
                offset += alignUp(sizes[i] * static_cast<size_t>(capacity), maxAlignment);

            return reinterpret_cast<std::tuple_element_t<Index, std::tuple<Ts...>>*>(
                reinterpret_cast<std::byte*>(this) + offset);
        }

        // Helper to iterate over tuple indices
        template <typename Tuple, std::size_t... I>
        void pushHostImpl(Tuple const& values, std::index_sequence<I...>)
        {
            ((buffer<I>()[tail] = std::get<I>(values)), ...);
        }

        template <typename Tuple, std::size_t... I>
        void popHostImpl(Tuple& out, std::index_sequence<I...>)
        {
            ((std::get<I>(out) = buffer<I>()[head]), ...);
        }

        bool pushHost(std::tuple<Ts...> const& values)
        {
            if (count >= capacity)
                return false;

            pushHostImpl(values, std::index_sequence_for<Ts...>{});
            tail = (tail + 1) % capacity;
            ++count;
            return true;
        }

        bool popHost(std::tuple<Ts...>* out)
        {
            if (count <= 0)
                return false;

            if (out)
                popHostImpl(*out, std::index_sequence_for<Ts...>{});

            head = (head + 1) % capacity;
            --count;
            return true;
        }

        // Helper to copy into tuple at slot
        template <typename Tuple, std::size_t... I>
        void peekHostImpl(Tuple& out, int slot, std::index_sequence<I...>)
        {
            ((std::get<I>(out) = buffer<I>()[slot]), ...);
        }

        bool peekHost(int index, std::tuple<Ts...>* out)
        {
            if (index < 0 || index >= count)
                return false;
            if (out)
            {
                int slot = (head + index) % capacity;
                peekHostImpl(*out, slot, std::index_sequence_for<Ts...>{});
            }
            return true;
        }
#endif

#if defined(__CUDA_ARCH__)

        // buffer pointer computation
        template <size_t Index>
        __device__ typename TypeAt<Index, Ts...>::type* buffer()
        {
            size_t sizes[]      = {sizeof(Ts)...};
            size_t alignments[] = {alignof(Ts)...};
            size_t maxAlign     = alignments[0];
            for (size_t i = 1; i < sizeof...(Ts); ++i)
                if (alignments[i] > maxAlign)
                    maxAlign = alignments[i];

            size_t offset = alignUp(sizeof(ManagedMultiQueue<Ts...>), maxAlign);
            for (size_t i = 0; i < Index; ++i)
                offset += alignUp(sizes[i] * static_cast<size_t>(capacity), maxAlign);

            using T = typename TypeAt<Index, Ts...>::type;
            return reinterpret_cast<T*>(reinterpret_cast<char*>(this) + offset);
        }

        // ------------------ Device push ------------------
        // pushDevice: values passed by const-ref
        template <typename... Us>
        __device__ bool pushDevice(Us const&... values_raw)
        {
            static_assert(sizeof...(Us) == sizeof...(Ts), "Wrong number of values");

            // pack the actual values into ValueTuple
            ValueTuple<Us...> vals(values_raw...);

            int ticket = atomicAdd(&tail, 1);
            int slot   = ticket % capacity;

            int oldCount = atomicAdd(&count, 1);
            if (oldCount >= capacity)
            {
                atomicSub(&count, 1);
                return false;
            }

            PushDeviceFunctor<ManagedMultiQueue<Ts...>, ValueTuple<Us...>> f(this, vals, slot);
            Indices<sizeof...(Ts)>::apply(f);
            return true;
        }


        // ------------------ Device pop ------------------
        // popDevice: outs are pointers to host-scoped locals (on device call use &localVar)
        template <typename... Us>
        __device__ bool popDevice(Us*... outs_raw)
        {
            static_assert(sizeof...(Us) == sizeof...(Ts), "Wrong number of outs");

            PtrTuple<Us...> outs(outs_raw...);
            int             ticket = atomicAdd(&head, 1);
            int             slot   = ticket % capacity;

            int oldCount = atomicSub(&count, 1);
            if (oldCount <= 0)
            {
                atomicAdd(&count, 1);
                return false;
            }

            PopDeviceFunctor<ManagedMultiQueue<Ts...>, PtrTuple<Us...>> f(this, outs, slot);
            Indices<sizeof...(Ts)>::apply(f);
            return true;
        }

#endif
    };

} // namespace dmt
